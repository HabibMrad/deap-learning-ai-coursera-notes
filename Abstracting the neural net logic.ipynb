{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "A^{[0]} &:= X \\\\\n",
    "\\downarrow \\\\\n",
    "Z^{[1]} &= W^{[1]}A^{[0]} + b^{[1]} \\\\\n",
    "A^{[1]} &= g^{[1]}(Z^{[1]}) \\\\\n",
    "\\downarrow \\\\\n",
    "Z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "A^{[2]} &= g^{[2]}(Z^{[2]}) \\\\\n",
    "\\downarrow \\\\\n",
    "\\vdots \\\\\n",
    "\\downarrow \\\\\n",
    "Z^{[L]} &= W^{[L]}A^{[L-1]} + b^{[L]} \\\\\n",
    "A^{[L]} &= g^{[L]}(Z^{[L]}) \\\\\n",
    "\\downarrow \\\\\n",
    "\\hat{Y} &:= A^{[L]}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\delta Z^{[l]}}{\\delta W^{[l]}} = A^{[l-1]} \\text{ and } \\frac{\\delta Z^{[l]}}{\\delta b^{[l]}} = 1 \\\n",
    "\\text{ and } \\frac{\\delta Z^{[l]}}{\\delta A^{[l-1]}} = W^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sigmoid non-linear function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{sigmoid}^{[l]} = \\frac{1}{1 + e^{-Z^{[l]}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\delta A_{sigmoid}^{[l]}}{\\delta Z^{[l]}} = A_{sigmoid}^{[l]}(1-A_{sigmoid}^{[l]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ReLU non-linear function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{relu}^{[l]} = \\max{(0, Z^{[l]})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\delta A_{relu}^{[l]}}{\\delta Z^{[l]}} = \\\n",
    "\\begin{cases}\n",
    "0 &\\quad\\text{if } Z^{[l]} < 0 \\\\ \n",
    "1 &\\quad\\text{if } Z^{[l]} \\geq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J = -\\frac{1}{m} \\Big( Y \\cdot \\log{(\\hat{Y})} + (1-Y) \\cdot \\log{(1-\\hat{Y})} \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} = -\\frac{1}{m} \\Big( \\frac{-Y}{\\hat{Y}} + \\frac{1-Y}{1-\\hat{Y}} \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want know know how the cost changes with respect to the weights and bias of the final layer $L$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\delta J}{\\delta W^{[L]}} &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta W^{[L]}} \\\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b^{[L]}} &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta b^{[L]}} \\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems, we will use the sigmoid activation function in the final layer, so we can abstract the common factor $dZ^{[L]} = \\frac{\\delta J}{\\delta Z^{[L]}}$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\n",
    "dZ^{[L]} = \\frac{\\delta J}{\\delta Z^{[L]}} &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}}\n",
    "\\\\\n",
    "&= \\\n",
    "\\frac{\\delta J}{\\delta A_{sigmoid}^{[L]}} \\cdot \\\n",
    "\\frac{\\delta A_{sigmoid}^{[L]}}{\\delta Z^{[L]}} \\\\\n",
    "&= \\\n",
    "-\\frac{1}{m} \\Big( \\frac{-Y}{A_{sigmoid}^{[L]}} + \\frac{1-Y}{1-A_{sigmoid}^{[L]}} \\Big)\n",
    "\\Big( A_{sigmoid}^{[L]}(1-A_{sigmoid}^{[L]}) \\Big) \\\\\n",
    "&= \\\n",
    "\\frac{1}{m} \\Big(A_{sigmoid}^{[L]} - Y \\Big)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know how the cost changes with respect to the weights of the layer before the final:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\delta J}{\\delta W^{[L-1]}} &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta A^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta A^{[L-1]}}{\\delta Z^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta  Z^{[L-1]}}{\\delta W^{[L-1]}}\n",
    "\\\\\n",
    "\\frac{\\delta J}{\\delta b^{[L-1]}} &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta A^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta A^{[L-1]}}{\\delta Z^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta  Z^{[L-1]}}{\\delta b^{[L-1]}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can abstract the common factor $dZ^{[L-1]} = \\frac{\\delta J}{\\delta Z^{[L-1]}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\n",
    "dZ^{[L-1]} = \\frac{\\delta J}{\\delta Z^{[L-1]}}  &= \\\n",
    "\\frac{\\delta J}{\\delta \\hat{Y}} \\cdot \\\n",
    "\\frac{\\delta \\hat{Y}}{\\delta Z^{[L]}} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta A^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta A^{[L-1]}}{\\delta Z^{[L-1]}}\n",
    "\\\\\n",
    "&= \\\n",
    "dZ^{[L]} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta A^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta A^{[L-1]}}{\\delta Z^{[L-1]}}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a $ReLU$ activation function for all the hidden layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "dZ^{[L-1]} &= \\\n",
    "dZ^{[L]} \\cdot \\\n",
    "\\frac{\\delta Z^{[L]}}{\\delta A^{[L-1]}} \\cdot \\\n",
    "\\frac{\\delta A_{relu}^{[L-1]}}{\\delta Z^{[L-1]}} \\\\ \\\\\n",
    "&= \\\n",
    "W^{[L]^T}dZ^{[L]}  \\cdot \\\n",
    "\\\n",
    "\\begin{cases}\n",
    "0 &\\quad\\text{if } Z^{[L-1]} < 0 \\\\ \n",
    "1 &\\quad\\text{if } Z^{[L-1]} \\geq 0\n",
    "\\end{cases} \n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be generalized as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "dZ^{[l]} = \\\n",
    "W^{[l+1]^T} dZ^{[l+1]} \\cdot \\\n",
    "\\begin{cases}\n",
    "0 &\\quad\\text{if } Z^{[l]} < 0 \\\\ \n",
    "1 &\\quad\\text{if } Z^{[l]} \\geq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and we need to initialize our parameters (weights and biases)\n",
    "def initialize_parameters(dimensions, seed = None):\n",
    "    if (seed): np.random.seed(seed)\n",
    "        \n",
    "    W = {}\n",
    "    b = {}\n",
    "    \n",
    "    for l in range(1, len(dimensions)):\n",
    "        W[str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.01\n",
    "        b[str(l)] = np.zeros((dimensions[l], 1))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can fit a model to the $NOT$ logical operation (denoted $\\neg$):\n",
    "\n",
    "$$\n",
    "\\begin{array}{ c c c }\n",
    " X & Y = \\neg{X} \\\\ \n",
    " \\hline\n",
    " 0 & 1 \\\\  \n",
    " 1 & 0    \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our inputs and outputs are scalar. Let's try to fit the $NOT$ function without a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[0 1]]\n",
      "Y:  [[1 0]]\n",
      "W:  {'2': array([[-0.01072969,  0.00865408, -0.02301539]]), '1': array([[ 0.01624345],\n",
      "       [-0.00611756],\n",
      "       [-0.00528172]])}\n",
      "b:  {'2': array([[ 0.]]), '1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "layer_dimensions = [1,3,1]\n",
    "\n",
    "X = np.array([[0,1]])\n",
    "Y = 1 - X\n",
    "print('X: ', X)\n",
    "print('Y: ', Y)\n",
    "\n",
    "W, b = initialize_parameters(layer_dimensions)\n",
    "\n",
    "print('W: ', W)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "relu = lambda z: np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(X,W,b):\n",
    "    Z = {}\n",
    "    A = {}\n",
    "    \n",
    "    L = len(W)\n",
    "    \n",
    "    a = X\n",
    "    A['0'] = a\n",
    "    \n",
    "    for l in [str(i+1) for i in range(L)]:\n",
    "        z = np.dot(W[l], a) + b[l]\n",
    "        a = relu(z) if l != str(L) else sigmoid(z)\n",
    "    \n",
    "        Z[str(l)] = z\n",
    "        A[str(l)] = a\n",
    "    \n",
    "    return a, Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "YHat, Z, A = forward(X,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = lambda Y, YHat: (-1/Y.shape[1]) * np.sum(Y * np.log(YHat) + (1-Y) * np.log(1-YHat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931036106682773"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(Y, YHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward(Y, A, Z, W):\n",
    "    dZ = {}\n",
    "    \n",
    "    L = len(W)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    dz = (A[str(L)] - Y) / m\n",
    "    dZ[str(L)] = dz\n",
    "    \n",
    "    for l in reversed([i+1 for i in range(L-1)]):\n",
    "        dz = np.dot(W[str(l+1)].T,dz) * np.array(Z[str(l)] > 0)\n",
    "        dZ[str(l)] = dz\n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': array([[ 0.        , -0.00268219],\n",
       "        [-0.        ,  0.        ],\n",
       "        [ 0.        , -0.        ]]), '2': array([[-0.25      ,  0.24997821]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ = backward(Y, A, Z, W)\n",
    "dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numeric_gradient(X, fn, epsilon = 1e-7):\n",
    "    gradients = np.zeros(X.shape)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            plus, minus = [np.copy(X), np.copy(X)]\n",
    "            plus[i,j] += epsilon\n",
    "            minus[i,j] -= epsilon\n",
    "            gradients[i,j] = (fn(plus) - fn(minus)) / (2 * epsilon)\n",
    "            \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = lambda a, b: np.sqrt(np.sum((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25        0.24997821]]\n",
      "[[-0.25        0.24997821]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0192216675912322e-10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ2_n = numeric_gradient(Z['2'], lambda Z2: cost(Y, sigmoid(Z2)))\n",
    "print(dZ['2'])\n",
    "print(dZ2_n)\n",
    "distance(dZ2_n, dZ['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.00268219]\n",
      " [-0.          0.        ]\n",
      " [ 0.         -0.        ]]\n",
      "[[ 0.00134121 -0.00268219]\n",
      " [-0.00108176  0.        ]\n",
      " [ 0.00287692  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0033534664943288993"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ1_n = numeric_gradient(Z['1'], lambda Z1: cost(Y, sigmoid(np.dot(W['2'], relu(Z1)) + b['2'])))\n",
    "print(dZ['1'])\n",
    "print(dZ1_n)\n",
    "distance(dZ1_n, dZ['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.49995643]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(W['2'], relu(Z['1'])) + b['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(dZ, A, W, b, learning_rate):\n",
    "    dW = {}\n",
    "    dB = {}\n",
    "    \n",
    "    _W = {}\n",
    "    _b = {}\n",
    "    \n",
    "    for l in [i+1 for i in range(len(W))]:\n",
    "        dw = np.dot(dZ[str(l)],A[str(l-1)].T)\n",
    "        db = np.sum(dZ[str(l)], axis=1, keepdims=True)\n",
    "\n",
    "        _W[str(l)] = W[str(l)] - dw * learning_rate\n",
    "        _b[str(l)] = b[str(l)] - db * learning_rate\n",
    "        \n",
    "        dW[str(l)] = dw\n",
    "        dB[str(l)] = db\n",
    "    \n",
    "    return _W, _b, dW, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, b, dW, db = update_parameters(dZ, A, W, b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69307720763727243"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YHat, Z, A = forward(X,W,b)\n",
    "cost(Y, YHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X,Y,shape, epochs, alpha, seed=None):\n",
    "    W, b = initialize_parameters(shape, seed=seed)\n",
    "\n",
    "    for i in range(epochs):    \n",
    "        YHat, Z, A = forward(X,W,b)\n",
    "        if (i % (epochs / 10) == 0): print(\"cost at iteration {}: {}\".format(i, cost(Y, YHat)))\n",
    "        dZ = backward(Y, A, Z, W)\n",
    "        W, b, dW, db = update_parameters(dZ, A, W, b, alpha)\n",
    "\n",
    "    print(\"cost at iteration {}: {}\".format(epochs, cost(Y, YHat)))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0: 0.6931257326865512\n",
      "cost at iteration 10000: 0.00011178431622663637\n",
      "cost at iteration 20000: 5.5350795198765576e-05\n",
      "cost at iteration 30000: 3.6726630636644556e-05\n",
      "cost at iteration 40000: 2.746135660596447e-05\n",
      "cost at iteration 50000: 2.1920918594980274e-05\n",
      "cost at iteration 60000: 1.823660827398187e-05\n",
      "cost at iteration 70000: 1.5609800102928724e-05\n",
      "cost at iteration 80000: 1.3642830748872628e-05\n",
      "cost at iteration 90000: 1.2114909282190857e-05\n",
      "cost at iteration 100000: 1.089418062271606e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,1]])\n",
    "Y = 1 - X\n",
    "\n",
    "W, b = model(X, Y, shape=[1,2,1], epochs=100000, alpha=1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat, _, _ = forward(X, W, b)\n",
    "np.round(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ c c c }\n",
    " X_1 & X_2 & Y = X_1 \\land X_2 \\\\ \n",
    " \\hline\n",
    " 0 & 0 & 0 \\\\ \n",
    " 0 & 1 & 0 \\\\\n",
    " 1 & 0 & 0 \\\\\n",
    " 1 & 1 & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0: 0.6931537991507776\n",
      "cost at iteration 10000: 0.00012141409013251916\n",
      "cost at iteration 20000: 5.9705588167839904e-05\n",
      "cost at iteration 30000: 3.9479310820581085e-05\n",
      "cost at iteration 40000: 2.9453636207564906e-05\n",
      "cost at iteration 50000: 2.3473901749588015e-05\n",
      "cost at iteration 60000: 1.9503307652577147e-05\n",
      "cost at iteration 70000: 1.6676702553356963e-05\n",
      "cost at iteration 80000: 1.4562793283211957e-05\n",
      "cost at iteration 90000: 1.2922315760005122e-05\n",
      "cost at iteration 100000: 1.1612775004161222e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1],\n",
    "]).T\n",
    "\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "]).T\n",
    "\n",
    "W, b = model(X, Y, shape=[2,2,1], epochs=100000, alpha=1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat, _, _ = forward(X, W, b)\n",
    "np.round(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ c c c }\n",
    " X_1 & X_2 & Y = X_1 \\lor X_2 \\\\ \n",
    " \\hline\n",
    " 0 & 0 & 0 \\\\ \n",
    " 0 & 1 & 1 \\\\\n",
    " 1 & 0 & 1 \\\\\n",
    " 1 & 1 & 1\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0: 0.6931186561290182\n",
      "cost at iteration 5000: 0.0002409086021001038\n",
      "cost at iteration 10000: 0.00011800414334797501\n",
      "cost at iteration 15000: 7.792100725693064e-05\n",
      "cost at iteration 20000: 5.809529269374713e-05\n",
      "cost at iteration 25000: 4.628038947343695e-05\n",
      "cost at iteration 30000: 3.844328283175966e-05\n",
      "cost at iteration 35000: 3.286725208232808e-05\n",
      "cost at iteration 40000: 2.8697095795395653e-05\n",
      "cost at iteration 45000: 2.5461732788293928e-05\n",
      "cost at iteration 50000: 2.2879928457563323e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1],\n",
    "]).T\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "]).T\n",
    "\n",
    "W, b = model(X, Y, shape=[2,2,1], epochs=50000, alpha=1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat, _, _ = forward(X, W, b)\n",
    "np.round(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ c c c }\n",
    " X_1 & X_2 & Y = X_1 \\oplus X_2 \\\\ \n",
    " \\hline\n",
    " 0 & 0 & 0 \\\\ \n",
    " 0 & 1 & 1 \\\\\n",
    " 1 & 0 & 1 \\\\\n",
    " 1 & 1 & 0\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0: 0.693152806790345\n",
      "cost at iteration 5000: 0.0003488680224083522\n",
      "cost at iteration 10000: 0.00016446249766163183\n",
      "cost at iteration 15000: 0.0001067492705668156\n",
      "cost at iteration 20000: 7.87403169220654e-05\n",
      "cost at iteration 25000: 6.225787299858608e-05\n",
      "cost at iteration 30000: 5.1415353815313e-05\n",
      "cost at iteration 35000: 4.375306426885527e-05\n",
      "cost at iteration 40000: 3.8056055150617013e-05\n",
      "cost at iteration 45000: 3.365631509024209e-05\n",
      "cost at iteration 50000: 3.0155874466667106e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1],\n",
    "]).T\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "]).T\n",
    "\n",
    "W, b = model(X, Y, shape=[2,3,1], epochs=50000, alpha=1, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat, _, _ = forward(X, W, b)\n",
    "np.round(yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
